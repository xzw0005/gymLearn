{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print \"Episode finished after {} timesteps\".format(t+1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "highscore = 0\n",
    "for i in range(20):    # run 20 episodes\n",
    "    observation = env.reset()\n",
    "    points = 0    # keep track of reward each episode\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = 1 if observation[2] > 0 else 0    # if angle is positive, move right; if angle is negative, move left\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        points += reward\n",
    "        if done:\n",
    "            if points > highscore:\n",
    "                highscore = points\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Table Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-08 22:52:29,620] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "# Set learning parameters\n",
    "alpha = .8    # learning rate\n",
    "gamma = .95   # discount factor\n",
    "num_episodes = 2000\n",
    "# Create lists to contain total rewards and steps per episode\n",
    "rewardsList = []\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    done = False\n",
    "    j = 0\n",
    "    while j < 99:\n",
    "        j+= 1\n",
    "        # Greedy action pick from Q table\n",
    "        a = np.argmax(Q[s, :] + np.random.randn(1, env.action_space.n)*1. / (i+1))\n",
    "        # Get new state and reward from environment\n",
    "        sprime, reward, done, info = env.step(a)\n",
    "        # Update Q-Table with new knowledge\n",
    "        Q[s, a] = Q[s, a] + alpha * (r + gamma * np.max(Q[sprime, :]) - Q[s, a])\n",
    "        rAll += reward\n",
    "        s = sprime\n",
    "        if done == True:\n",
    "            break\n",
    "        rewardsList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.5625\n",
      "Final Q-Table Values\n",
      "[[  1.59598640e-01   1.76867872e-02   1.53506889e-02   1.58404103e-02]\n",
      " [  6.28693462e-04   3.62018234e-03   2.54869079e-03   1.47798279e-01]\n",
      " [  2.99186428e-03   2.48337779e-03   2.46187372e-03   2.38263743e-01]\n",
      " [  9.62295821e-04   8.94648050e-04   2.32334161e-03   8.47611637e-02]\n",
      " [  1.62679850e-01   3.36581467e-03   0.00000000e+00   3.19434265e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.49438813e-01   3.19699316e-05   1.22139050e-04   2.87185626e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  4.26831948e-03   6.76297273e-03   7.71637220e-03   1.22685603e-01]\n",
      " [  4.14003269e-04   3.37345201e-01   2.14859481e-03   0.00000000e+00]\n",
      " [  5.87458741e-01   2.01174262e-03   5.03528303e-04   4.25338158e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.12801720e-02   0.00000000e+00   8.09217313e-01   3.27248749e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   9.97626452e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print \"Score over time: \" +  str(sum(rList)/num_episodes)\n",
    "print \"Final Q-Table Values\"\n",
    "print Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
